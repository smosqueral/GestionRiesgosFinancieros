---
title: "Gesti√≥n de Riesgos Financieros"
author: "Stephan√≠a Mosquera L√≥pez"
date: "2/07/2022"
output:
    html_document:
      toc: true
      toc_float: true
      toc_depth: 3
---

# Riesgo de Mercado

## Retornos Financieros

Este documento es un [R Markdown](http://rmarkdown.rstudio.com) Notebook y ser√° el formato en el que tendremos las notas de clase. 

Vamos a arrancar descargando e instalando `R` siguiendo estas instrucciones:

1. Descargar R de la p√°gina web http://cran.r-project.org/.
2. Aqu√≠ pueden descargar `R` para Linux, Mac o Windows.
3. Una vez descargada la versi√≥n m√°s actualizada del software, ejecute el instalador. 

Despu√©s de que est√© instalado `R`, continuamos con la instalaci√≥n de `RStudio`, que es la interfraz gr√°fica que vamos a utilizar. Las instrucciones son:

1. Descargar RStudio de la p√°gina web https://www.rstudio.com/products/RStudio/.
2. Dar click en la pesta√±a *Products*,`RStudio`. 
3. Descargar *RStudio Desktop*, Open Source Edition, que es la versi√≥n gratuita. 

Una vez instalado `RStudio` vamos a instalar `R Markdown`, que es una librer√≠a que les permitir√° crear documentos como este, donde pueden comentar de una manera m√°s organizada los c√≥digos que escriban. Tambi√©n pueden crear p√°ginas web, dashboards, PDFs, documentos de Word, entre otros.  

En la consola escriba:

```{r warning=FALSE}
#install.packages("rmarkdown")
library(rmarkdown)
```

En la p√°gina web  http://rmarkdown.rstudio.com/gallery.html pueden encontrar ejemplos y templates de documentaci√≥n realizada con `R Markdown.

### Obtener informaci√≥n de FRED Data

La Reserva Federal del Banco de St.Lois mantiene la base de datos p√∫blica llamada **FRED**, que contiene informaci√≥n de:

- Variables macroecon√≥micas: PIB, desempleo, oferta de dinero, ...
- Tasas de cambio
- Tasas de inter√©s
- Precios de commodities
- √çndices accionarios

Se puede descargar la informaci√≥n desde la p√°gina web https://fred.stlouisfed.org/ o directamente desde `R`, as√≠:

```{r warning=FALSE,message=FALSE}
library(quantmod)
SP <- getSymbols("SP500",src="FRED", auto.assign=FALSE)
SP <- na.omit(SP)
```
Para saber si cargamos la informaci√≥n adecuada podemos ver algunas l√≠neas de los datos.

**Del inicio:**
```{r}
head(SP,4)
```
**Del final:**
```{r}
tail(SP,4)
```

Ahora grafiquemos el √≠ndice: 
```{r warning=TRUE}
plot(SP)
```

### C√°lculo de los Retornos

El **retorno discreto** es igual a:

$ret_{t}=\frac{SP_{t}-SP_{t-1}}{SP_{t-1}}$

Mientras que el **retorno continuo** lo definimos como:

$logret_{t}=log(1+ret_{t})=log(SP_{t})-log(SP_{t-1})=log(\frac{SP_{t}}{SP_{t-1}})$

```{r}
logret <-diff(log(SP))
logret <-na.omit(logret)
head(logret,4)
```
**Gr√°fica de los retornos logar√≠tmicos** 

```{r warning=TRUE}
plot(logret)
```

Si queremos pasar del retorno logar√≠tmico al retorno discreto lo podemos hacer as√≠: $ret_{t}=exp(logret_{t})-1$

```{r}
ret <-exp(logret)-1
head(ret,4)
```
**Pregunta: ¬øQu√© tan parecidos son los retornos discretos con los logar√≠tmicos? ¬øPor qu√© preferimos los logar√≠tmicos?**

**Horizontes de tiempo m√°s largos**

Tambi√©n podemos calcular los retornos para horizontes de tiempo diferentes al diario. 

**De retorno logar√≠tmico a 1-d√≠a a retorno logar√≠tmico a n-d√≠as:**

- Empiece con el retorno logar√≠tmico  a 1-d√≠a, de $t-1$ a $t$:$logret_{t}$
- Calcule el retorno logar√≠tmico a n-d√≠as, de $t-n$ a $t$: $logret_{t}+...+logret_{t-n+1}$
- Calcule el retorno discreto a n-d√≠as: $exp(logret_{t}+...+logret_{t-n+1})-1$

**Estimaci√≥n de los retornos semanales**

```{r}
logret_w <- apply.weekly(logret,sum)
head(logret_w,4)
```

`R` tambi√©n tiene funciones para:

- Retornos mensuales:    `apply.monthly`
- Retornos trimestrales: `apply.quarterly`
- Retornos anuales:      `apply.yearly`     

### Hechos Estilizados

**1. Baja autocorrelaci√≥n de los retornos diarios**

```{r}
pacf(logret)
```

**2. La distribuci√≥n de los retornos no es normal**

```{r}
hist(logret, freq=FALSE,breaks=20, col="gray")
lines(seq(-0.1,0.05, by=0.001), dnorm(seq(-0.1,0.05, by=0.001),
      mean(logret), sd(logret)), col="blue")
```
**3. Clusters de Volatilidad**

```{r warning=TRUE}
ret2 = logret^2
pacf(ret2)

```

## Repaso Estad√≠stica

Primero cargamos las librer√≠as que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')
```
Ahora vamos a descargar el S&P 500, las primeras 10 acciones del √≠ndice con mayor capitalizaci√≥n burs√°til a enero de 2022 y un bono soberano estadounidense: 

```{r warning=FALSE, message=FALSE}
#S&P500
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp1<-na.locf(GSPC$GSPC.Adjusted)  

#1.Apple
getSymbols("AAPL",src='yahoo', from= startt , to = endd);sp2<-na.locf(AAPL$AAPL.Adjusted) 

#2.Microsoft
getSymbols("MSFT",src='yahoo', from= startt , to = endd); sp3<-na.locf(MSFT$MSFT.Adjusted)  

#3.Amazon
getSymbols("AMZN",src='yahoo', from= startt , to = endd); sp4<-na.locf(AMZN$AMZN.Adjusted)  

#4.Alphabet Class A
getSymbols("GOOGL",src='yahoo', from= startt , to = endd); sp5<-na.locf(GOOGL$GOOGL.Adjusted)

#5.Tesla
getSymbols("TSLA",src='yahoo', from= startt , to = endd); sp6<-na.locf(TSLA$TSLA.Adjusted) 

#6.Alphabet Class C
getSymbols("GOOG",src='yahoo', from= startt , to = endd); sp7<-na.locf(GOOG$GOOG.Adjusted)

#7.Meta
getSymbols("FB",src='yahoo', from= startt , to = endd); sp8<-na.locf(FB$FB.Adjusted) 

#8.NVIDIA Corporation
getSymbols("NVDA",src='yahoo', from= startt , to = endd); sp9<-na.locf(NVDA$NVDA.Adjusted)

#9.Berkshire Hathaway Inc
getSymbols("BRK-B",src='yahoo', from= startt , to = endd); sp10<-na.locf(`BRK-B`$`BRK-B.Adjusted`) 

#UnitedHealth Group
getSymbols("UNH",src='yahoo', from= startt , to = endd); sp11<-na.locf(UNH$UNH.Adjusted)

#Tbill 13 weeks
getSymbols("^IRX",src='yahoo', from= startt , to = endd); sp12<-na.locf(IRX$IRX.Adjusted)     
```
En la siguiente parte unimos todas las series en una misma matriz:

```{r warning=FALSE}
sp <- merge(sp1,sp2,fill=na.locf)
sp <- merge(sp,sp3,fill=na.locf)
sp <- merge(sp,sp4,fill=na.locf)
sp <- merge(sp,sp5,fill=na.locf)
sp <- merge(sp,sp6,fill=na.locf)
sp <- merge(sp,sp7,fill=na.locf)
sp <- merge(sp,sp8,fill=na.locf)
sp <- merge(sp,sp9,fill=na.locf)
sp <- merge(sp,sp10,fill=na.locf)
sp <- merge(sp,sp11,fill=na.locf)
sp <- merge(sp,sp12,fill=na.locf)

sm<-sp
```
### Precios en Niveles

**Gr√°fica de los Precios en Niveles:**

```{r, warning=FALSE, message=FALSE}

par(mfrow=c(3,1))
plot(sm[,1], main ="SP500")
plot(sm[,2], main ="Apple")
plot(sm[,3], main ="Microsoft")
plot(sm[,4], main ="Amazon")
plot(sm[,5], main ="Alphabet Class A")
plot(sm[,6], main ="Tesla")
plot(sm[,7], main ="Alphabet Class C")
plot(sm[,8], main ="Meta")
plot(sm[,9], main ="NVIDIA")
plot(sm[,10], main ="Berkshire Hathaway")
plot(sm[,11], main ="United Health Group")
plot(sm[,12], main ="Tbill 13 weeks")

```
Calculemos las estad√≠sticas descriptivas de los precios:

```{r warning=FALSE}
m <- ncol(sm)
EstDesPrecios <- matrix(,7,m)

for (i in 1:m){
  EstDesPrecios[1,i]=colMins(sm[,i])
  EstDesPrecios[2,i]=colMeans(sm[,i])
  EstDesPrecios[3,i]=colMaxs(sm[,i])
  EstDesPrecios[4,i]=colVars(sm[,i])
  EstDesPrecios[5,i]=colSds(sm[,i])
  EstDesPrecios[6,i]=colSkewness(sm[,i])
  EstDesPrecios[7,i]=colKurtosis(sm[,i])
}

rownames(EstDesPrecios) <- c("Min","Mean","Max","Variance","Sds","Skewness","Kurtosis")
colnames(EstDesPrecios) <- c("SP500","Apple","Microsoft","Amazon","AlphabetA","Tesla","AlphabetC","Meta","NVIDIA","BerkshireHathaway","UnitedHealth","Tbill")

EstDesPrecios %>%
  kable(digits=4) %>%
  kable_styling("striped", full_width = F, position = "center")

```

### Retornos

```{r, warning=FALSE, message = FALSE}
ret<- diff(log(sm))*100
ret<-na.locf(ret)
ret[is.na(ret)] <- 0

```

**Gr√°fica de los Retornos:**

```{r}

par(mfrow=c(3,1))
plot(ret[,1], main ="SP500")
plot(ret[,2], main ="Apple")
plot(ret[,3], main ="Microsoft")
plot(ret[,4], main ="Amazon")
plot(ret[,5], main ="AlphabetA")
plot(ret[,6], main ="Tesla")
plot(ret[,7], main ="AlphabetB")
plot(ret[,8], main ="Meta")
plot(ret[,9], main ="NVIDIA")
plot(ret[,10], main ="Berkshire Hathaway")
plot(ret[,11], main ="United Health Group")
plot(ret[,12], main ="Tbill 13 weeks")

```
Calculemos ahora las estad√≠sticas descriptivas de los retornos:

```{r warning=FALSE}
m <- ncol(ret)
EstDesRetornos <- matrix(,8,12)

for (i in 1:m){
  EstDesRetornos[1,i]=colMins(ret[,i])
  EstDesRetornos[2,i]=colMeans(ret[,i])
  EstDesRetornos[3,i]=colMaxs(ret[,i])
  EstDesRetornos[4,i]=colVars(ret[,i])
  EstDesRetornos[5,i]=colSds(ret[,i])
  EstDesRetornos[6,i]=colSkewness(ret[,i])
  EstDesRetornos[7,i]=colKurtosis(ret[,i])
  EstDesRetornos[8,i]=jarque.bera.test(ret[,i])$p.value
}

rownames(EstDesRetornos) <- c("Min","Mean","Max","Variance","Sds","Skewness","Kurtosis","Jarque Bera")

colnames(EstDesRetornos) <- c("SP500","Apple","Microsoft","Amazon","AlphabetA","Tesla","AlphabetC","Meta","NVIDIA","BerkshireHathaway","UnitedHealth","Tbill")

EstDesRetornos %>%
  kable(digits=4) %>%
  kable_styling("striped", full_width = F, position = "center")

```
### Modelo CAPM

El Modelo de Valoraci√≥n de Activos de Capital (CAPM) es un modelo que proporciona una explicaci√≥n de la prima por riesgo, $\mu_{j}-r{0}$, de cada activo. 

Es una extensi√≥n del modelo de media-varianza para un solo inversionista al mercado completo. 

Es un modelo de equilibrio en el sentido tradicional: oferta = demanda. 

**Ecuaci√≥n del CAPM:**

$\mu_{j}=r_{0}+\beta_{j}(\mu_{M}-r_{0})$ para $j=1,2, ...,n$

Ahora procedemos a calcular los betas de las 10 acciones del SP500 con las que estamos trabajando:

```{r}
X<-as.matrix(ret[,1]-ret[,12])
YY<-ret[,2:11]
t<-dim(YY)[1]
n<-dim(YY)[2]
Y<-matrix(0,t,n)

for (i in 1:n){
  Y[,i]<-YY[,i]-ret[,12]}

Resultados <-matrix(0,n,4)

for (i in 1:n){
  Y2=Y[,i]
  olsreg <- lm(Y2~X)
  res <- summary(olsreg)
  Resultados[i,1:4] <- res$coef[2,1:4]}

rownames(Resultados) <- c("Beta Apple","Beta Microsoft","Beta Amazon","Beta Alphabet A","Beta Tesla","Beta AlphabetC","Beta Meta","Beta NVIDIA","Beta Berkshire Hathaway","Beta UnitedHealth")
colnames(Resultados) <- c("Estimate","Std.Error","t-value","p-value")

c("SP500","Apple","Microsoft","Amazon","AlphabetA","Tesla","AlphabetC","Meta","NVIDIA","BerkshireHathaway","UnitedHealth","Tbill")

Resultados %>%
  kable(digits=4) %>%
  kable_styling("striped",full_width = F, position = "center")
```

## Modelaci√≥n Media

Primero cargamos las librer√≠as que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(BatchGetSymbols)
library(lmtest)
library(forecast)
library(ggfortify)
library(FinTS)
library(timetk)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')
```
Descargamos el SP500:

```{r}
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp<-na.locf(GSPC$GSPC.Adjusted)
t <- nrow(sp)
date <- tk_index(sp)
```

### Paso 1. Estacionariedad

El primer paso de la metodolog√≠a Box-Jenkins para ajustar un modelo ARMA a una serie de tiempo es establecer la **estacionariedad** de √©sta.

As√≠, revisamos la estacionariedad de la serie utilizando la prueba de ra√≠z unitaria de Dickey-Fuller aumentada:

```{r}
adf.test(sp)
```

Como la serie no es estacionaria debemos diferenciarla y realizar nuevamente la prueba:

```{r}
ret <-diff(log(sp))
ret <-na.omit(ret)

adf.test(ret)
```
Gr√°fica de los retornos logar√≠tmicos: 

```{r}
plot(ret)
```

Una vez se realiza la inspecci√≥n gr√°fica de los datos debemos calcular las estad√≠sticas descriptivas de los datos y probar normalidad. 

```{r}
m <- ncol(ret)
EstDesRetornos <- matrix(,8,1)

  EstDesRetornos[1]=colMins(ret)
  EstDesRetornos[2]=colMeans(ret)
  EstDesRetornos[3]=colMaxs(ret)
  EstDesRetornos[4]=colVars(ret)
  EstDesRetornos[5]=colSds(ret)
  EstDesRetornos[6]=colSkewness(ret)
  EstDesRetornos[7]=colKurtosis(ret)
  EstDesRetornos[8]=jarque.bera.test(ret)$p.value
  
  rownames(EstDesRetornos) <- c("Min","Mean","Max","Variance","Sds","Skewness","Kurtosis","Jarque Bera")

colnames(EstDesRetornos) <- c("SP500")

EstDesRetornos %>%
    kable(digits=4) %>%
  kable_styling("striped", full_width = F, position = "center")

```


```{r}
hist(sp, freq = F)
curve(dnorm(x, mean(sp), sd(sp)),
      col = "blue", lwd = 3, add = TRUE)
```

### Paso 2. Identificaci√≥n

Procedemos a indentificar un modelo de la media condicional a los datos. Para estos nos basamos en la funci√≥n de autocorrelaci√≥n y la funci√≥n de autocorrelaci√≥n parcial.

```{r}
par(mfrow=c(2,1))
acf(ret, lag=24) 
pacf(ret, lag=24) 
```

Tambi√©n podemos realizar pruebas de autocorrelaci√≥n:

```{r}
Box.test(ret,lag=12,type="Box-Pierce") #H0: no autocorrelaci√≥n
Box.test(ret,lag=12,type="Ljung") #H0: no autocorrelaci√≥n
```
### Paso 3. Especificaci√≥n

El paso 3 consiste en especificar varios modelos y comparar mendiante criterios de selecci√≥n. 

#### AIC

```{r}

AIC <-matrix(0,6,6)

d=0

for (p in 0:5)  
{
  for (q in 0:5)  
  {
    modelo<-arima(ret,order=c(p,d,q))
    AIC[p+1,q+1]<-AIC(modelo)
     } 
 } 

rownames(AIC) <- c("AR(0)","AR(1)","AR(2)","AR(3)","AR(4)","AR(5)")
colnames(AIC) <- c("MA(0)","MA(1)","MA(2)","MA(3)","MA(4)","MA(5)")

AIC %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")

```

#### BIC

```{r}

BIC <-matrix(0,6,6)

d=0

for (p in 0:5)  
{
  for (q in 0:5)  
  {
    modelo<-arima(ret,order=c(p,d,q))
    BIC[p+1,q+1]<-BIC(modelo)
     } 
 } 

rownames(BIC) <- c("AR(0)","AR(1)","AR(2)","AR(3)","AR(4)","AR(5)")
colnames(BIC) <- c("MA(0)","MA(1)","MA(2)","MA(3)","MA(4)","MA(5)")

BIC %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")

```
Tambi√©n podemos utilizar la funci√≥n `auto.arima()` para escoger el mejor modelo. Esta funci√≥n hace una combinaci√≥n de pruebas de ra√≠z unitaria (para saber el orden de integreaci√≥n), minimizaci√≥n del criterio AIC y m√°ximizaci√≥n del log de verosimilitud.

Podemos utilizar esta funci√≥n para los precios en niveles o para los retornos. 

**Precios**

```{r}
auto.arima(sp, allowdrift = TRUE, allowmean = TRUE)
```
```{r}
modelo.precios = arima(sp, order=c(4,1,2))
coeftest(modelo.precios)

```

**Retornos**

```{r}
auto.arima(ret)
```

```{r}
modelo.retornos = arima(ret, order=c(2,0,2))
coeftest(modelo.retornos)

```

### Paso 4. Bondad de Ajuste

Debemos asegurarnos que los residuales no presenten autocorrelaci√≥n y que sean homoced√°sticos. 

```{r}
res.precios <- residuals(modelo.precios)
res.retornos <- residuals(modelo.retornos)
```
**Precios Autocorrelaci√≥n**

```{r}
par(mfrow=c(2,1)) 
acf(res.precios, lag=12) 
pacf(res.precios,lag=12)

Box.test(res.precios,lag=12,type="Box-Pierce") #H0: no autocorrelaci√≥n
Box.test(res.precios,lag=12,type="Ljung") #H0: no autocorrelaci√≥n
```

**Precios Heterocedasticidad**

```{r}
par(mfrow=c(2,1)) 
acf(res.precios^2, lag=12) 
pacf(res.precios^2,lag=12)

Box.test(res.precios^2,lag=12,type="Box-Pierce") #H0: no autocorrelaci√≥n
Box.test(res.precios^2,lag=12,type="Ljung") #H0: no autocorrelaci√≥n
```

**Retornos Autocorrelaci√≥n**

```{r}
par(mfrow=c(2,1)) 
acf(res.retornos, lag=12) 
pacf(res.retornos,lag=12)

Box.test(res.retornos,lag=12,type="Box-Pierce") #H0: no autocorrelaci√≥n
Box.test(res.retornos,lag=12,type="Ljung") #H0: no autocorrelaci√≥n
```

**Retornos Heterocedasticidad**

```{r}
par(mfrow=c(2,1)) 
acf(res.retornos^2, lag=12) 
pacf(res.retornos^2,lag=12)

Box.test(res.retornos^2,lag=12,type="Box-Pierce") #H0: no autocorrelaci√≥n
Box.test(res.retornos^2,lag=12,type="Ljung") #H0: no autocorrelaci√≥n
```

### Paso 5. Pronosticar

**Precios**

```{r}
forecast<- predict(modelo.precios, n.ahead=50)

U <- forecast$pred + 1.96*forecast$se 
L <- forecast$pred - 1.96*forecast$se 
Y <- forecast$pred
fecha <- tk_index(sp)
fecha2 <- fecha[1188:1237]
sp2 <- as.matrix(sp)

plot(fecha,sp2,type="l",col="black")
lines(fecha2,U, col="red",lty=2)
lines(fecha2,L, col="red",lty=2)
lines(fecha2,Y, col="blue",type="l")

```

**Retornos**

```{r}
forecast<- predict(modelo.retornos, n.ahead=50)

U <- forecast$pred + 1.96*forecast$se 
L <- forecast$pred - 1.96*forecast$se 
Y <- forecast$pred
fecha <- tk_index(ret)
fecha2 <- fecha[1188:1237]
ret2 <- as.matrix(ret)

plot(fecha,ret2,type="l",col="black")
lines(fecha2,U, col="red",lty=2)
lines(fecha2,L, col="red",lty=2)
lines(fecha2,Y, col="blue",type="l")

```

## Modelaci√≥n Varianza

Cargamos las librer√≠as que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(BatchGetSymbols)
library(lmtest)
library(forecast)
library(ggfortify)
library(FinTS)
library(timetk)
library(rugarch)
library(qrmtools)
library(MTS)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')

```
Descargamos el SP500:

```{r message=FALSE}
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp<-na.locf(GSPC$GSPC.Adjusted)
```
Calculamos los retornos logar√≠tmicos:

```{r warning=FALSE}
ret <-diff(log(sp))
ret <-na.omit(ret)
```

### Volatilidad Hist√≥rica

Ahora procedemos a calcular la volatilidad hist√≥rica:

```{r warning=FALSE}
m=252
sp.volh= rollapply(ret,m,sd)
plot(sp.volh)
```

### Volatilidad EWMA

```{r}
ewma=EWMAvol(ret,lambda = 0.94)
sp.volRM = ewma[["Sigma.t"]]
plot(sp.volRM, type="l")
```

### Volatilidad GARCH

1.Estimamos el **modelo ARMA** de los retornos:

```{r}
auto.arima(ret)
media =arima(ret, order=c(2,0,4))
res = residuals(media)

Box.test(res,lag=12,type="Box-Pierce") 
Box.test(res,lag=12,type="Ljung") 
```
2. Despu√©s de extraer los residuales del modelo en media, realizamos pruebas de **heterocedasticidad**. 

La prueba de Engle para efectos ARCH tiene como hip√≥tesis alterna que los errores al cuadrado est√°n correlacionados con los errores al cuadrado rezagados.

$H_{a}:e_{t}^2=\alpha_{0}+\alpha_{1}e_{t-1}^2+...+\alpha_{m}e_{t-m}^2+u_{t}$

$H_{0}:\alpha_{0}=\alpha_{1}=...=\alpha_{m}=0$

```{r}
ArchTest(res, lag=12)

Box.test(res^2,lag=12,type="Box-Pierce") 
Box.test(res^2,lag=12,type="Ljung")

par(mfrow=c(2,1))
acf(res^2, lag=24) 
pacf(res^2, lag=24)
```

3. Estimaci√≥n Modelo GARCH(1,1)

```{r}
Modelo <- garch(res,order=c(1,1))
summary(Modelo) 
```
Obtenemos el pron√≥stico de la varianza del modelo y estandarizamos los residuales del modelo en media:

```{r}
m <- nrow(ret)
res.garch <- fitted.values(Modelo)
res.garch <- res.garch[2:m,1]
res <- res[2:m]

res.est <- res/res.garch
```
4. Realizamos pruebas de bondad de ajuste sobre los nuevos residuales

```{r}
ArchTest(res.est, lag=12)

Box.test(res.est^2,lag=12,type="Box-Pierce") 
Box.test(res.est^2,lag=12,type="Ljung")

par(mfrow=c(2,1))
acf(res.est^2, lag=24) 
pacf(res.est^2, lag=24)
```

5. Por √∫ltimo, graficamos las tres varianzas que estimamos

```{r}
fecha <- tk_index(ret)
fecha <- fecha[2:m]

sp.volh2 <- as.matrix(sp.volh[2:m,1])
sp.volRM2 <- sqrt(sp.volRM[2:m,1])
sp.volGARCH <- res.garch

vol     <- matrix(,m-1,4)
vol[,1] <- sp.volh2
vol[,2] <- sp.volRM2
vol[,3] <- sp.volGARCH
vol[,4] <- as.matrix(sp[2:m,])

colnames(vol) <- c("VolHist", "VolRM", "VolGARCH", "SP500")
x= as.data.frame(vol)

graf <- ggplot(x,aes(x=fecha,y=VolHist)) + 
        geom_line(aes(y=VolHist), size = 0.25, col="red") +
        geom_line(aes(y=VolRM), size = 0.25, col="blue") +
        geom_line(aes(y=VolGARCH), size = 0.25, col="azure4") +             ylab('Volatilidad')+ theme_minimal() + theme(text = element_text(size=10), axis.text.x = element_text(size=10),axis.text.y = element_text(size=10))
        graf

```

## Estimaci√≥n del VaR y ES

### Valor en Riesgo

El **Valor en Riesgo (VaR)** y el **Expected Shortfall (ES)** son medidas que buscan resumir en un solo n√∫mero el riesgo total de un portafolio.

El **VaR** es una medida de la exposici√≥n de un portafolio de inversi√≥n al riesgo de mercado. Se difundi√≥ ampliamente despu√©s de su publicaci√≥n en un documento t√©cnico de **RiskMetrics** en 1994 para J.P. Morgan en Estados Unidos. Esta medida hoy constituye un paradigma financiero en el manejo de todo tipo de riesgos. El **VaR** se define como la **m√°xima p√©rdida** esperada (o la peor p√©rdida) que podr√≠a registrarse durante un determinado per√≠odo de tiempo, para un nivel dado de confianza.

Formalmente, si se tiene un **nivel de significancia** $\alpha$, y si $q_{(1-\alpha)}$ es el **cuantil** $(1-\alpha)$  de la distribuci√≥n de p√©rdidas para un **per√≠odo de tiempo**, el VaR de un portafolio a ese nivel de confianza y en ese per√≠odo de tiempo es igual a: 

$VaR_{(1-\alpha)}=q_{(1-\alpha)}(L)$, donde $L$ es la funci√≥n de p√©rdidas. 

De esta manera, existen dos par√°metros que determinan el **VaR** en principio: 

* **Per√≠odo de tiempo:** Per√≠odo durante el cual se mide la ganancia o p√©rdida del portafolio. Usualmente es a un d√≠a, 10 d√≠as, o un mes. 

* **Nivel de confianza:** T√≠picamente est√° entre 95% y 99%.

#### Limitaciones del VaR

El VaR es atractivo porque es f√°cil de entender. Nos dice qu√© tan mal pueden llegar a ser las cosas. Sin embargo, presenta las siguientes limitaciones:

1. Dice muy poco sobre los casos en las colas (p√©rdidas extremas).

2. Puede crear estructuras perversas de incentivos porque no se conoce la magnitud de las p√©rdidas que exceden las colas.

3. No es subaditivo, lo cual puede generar incentivos para construir portafolios demasiado concentrados.

### Expected Shortfall

El VaR no cumple con ser **subaditivo** (no cumple con este axioma de las medidas coherentes de riesgo), lo cual se suma a su pobre desempe√±o como medida de riesgo en situaciones extremas.Una alternativa te√≥rica para la estimaci√≥n del riesgo es el m√©todo de **p√©rdidas esperadas en las colas (ES)**. Se le conoce como el expected shortfall o VaR condicional. 

Se define el **ES** como el valor esperado de las p√©rdidas, ùêø, en caso tal que √©stas sean superiores al **VaR**: $ES=E[L|L>VaR]$. Las formas de calcular este valor esperado difieren seg√∫n el m√©todo utilizado para calcular el **VaR subyacente**.

A continuaci√≥n veremos cuatro **metodolog√≠as** para calcular el **VaR** y **ES**.

### M√©todo Simulaci√≥n Hist√≥rica

El m√©todo de simulaci√≥n hist√≥rica (HS por sus siglas en ingl√©s) es un m√©todo no param√©trico, pues no supone que los datos se distribuyen con una funci√≥n de probabilidad te√≥rica espec√≠fica, sino que aproxima el VaR mediante la funci√≥n de distribuci√≥n emp√≠rica de los datos. **HS** utiliza los datos pasados como gu√≠a de lo que va a pasar en el futuro. 

#### Algoritmo de estimaci√≥n: 

1. Escoja una secuencia pasada de ùíé retornos diarios hipot√©ticos. 

2. Construya de la funci√≥n de p√©rdidas: los retornos son ordenados de mayor a menor y multiplicados por menos uno. 
3. El **VaR** es igual a: $VaR_{(1-\alpha)}=q_{(1-\alpha)}(L)$, donde $q_{(1-\alpha)}$ es el percentil $(1-\alpha)$ de la funci√≥n de p√©rdidas definida como $L$.

4. El **ES** es igual al promedio de las p√©rdidas que se encuentran por encima del **VaR**.

#### Estimaci√≥n

Cargamos las librer√≠as que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(BatchGetSymbols)
library(lmtest)
library(forecast)
library(ggfortify)
library(FinTS)
library(timetk)
library(rugarch)
library(qrmtools)
library(MTS)
library(ismev)
```
Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2016/01/01')
endd=c('2021/12/01')

```
Descargamos el SP500:

```{r message=FALSE}
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp<-na.locf(GSPC$GSPC.Adjusted)
```
Calculamos los retornos logar√≠tmicos:

```{r warning=FALSE}
ret <-diff(log(sp))
ret <-na.omit(ret)
```
Ahora definimos la ventana de tiempo que vamos a utilizar ($m$), el nivel de confianza $(1-\alpha)$ y la funci√≥n de p√©rdidas. Recuerden que entre m√°s grande sea la ventana de tiempo mayor persistencia tendr√° la medida de riesgo estimada.

```{r}
m = nrow(ret)
alpha = 0.05
L = ret*-1
```

As√≠, el **VaR** por **HS** es igual a:

```{r}
VaRHist = quantile(L,(1-alpha))
VaRHist = as.numeric(VaRHist)
```

El **ES** por **HS** ser√≠a igual a:

```{r}
L <- as.data.frame(L)
ESHist <- filter(L,L>VaRHist)%>%
  summarise(mean(`GSPC.Adjusted`))
ESHist <- as.numeric(ESHist)

```

Para las siguientes tres metodolog√≠as debemos hacer la distinci√≥n entre un VaR/ES **est√°tico** y uno **condicional**. El primero asume la media y la varianza de los retornos como constantes durante el per√≠odo de an√°lisis, mientras que el segundo asume que son din√°micos. 

#### C√°lculo Media

Primero, calculamos la media est√°tica:

```{r}
L = ret*-1
miu = mean(L)
```
Segundo, calculamos la media condicional, que es el pron√≥stico del d√≠a siguiente que obtenemos de un modelo ARIMA:

```{r}
auto.arima(L)
modelo.retornos = arima(ret, order=c(1,0,4))
coeftest(modelo.retornos)
forecast<- predict(modelo.retornos, n.ahead=1)

miut1 <- as.numeric(forecast[["pred"]])
```
#### C√°lculo Volatilidad

Primero, calculamos la volatilidad est√°tica:

```{r}
sigma = sqrt(var(L))
```
Segundo, calculamos la volatilidad condicional, que es el pron√≥stico del d√≠a siguiente que obtenemos de un modelo GARCH o EWMA. Usualmente, un GARCH(1,1) es suficiente:

```{r}
res = residuals(modelo.retornos)
modelo.vol <- garch(res,order=c(1,1))
summary(modelo.vol)

forecast.vol <- predict(modelo.vol,n.head=1) 

sigmat1 <- as.numeric(sqrt(forecast.vol[2,1])) 
```
### M√©todo de Normalidad

#### Est√°tico

Al ser el **VaR** un cuantil de la funci√≥n de p√©rdidas, una forma de calcularlo param√©tricamente es suponiendo que √©sta sigue una **distribuci√≥n normal**: 

$VaR_{(1-\alpha)}=\mu + \sigma\Phi^{-1}(1-\alpha)$

Donde $(1-\alpha)$ es el nivel de confianza, $\Phi^{-1}$ es la inversa generalizada de la funci√≥n de distribuci√≥n normal est√°ndar (o funci√≥n cuantil), $\mu$ es una estimaci√≥n de la media y $\sigma$ de la desviaci√≥n est√°ndar.

As√≠, el **VaR est√°tico** por el m√©todo de normalidad ser√≠a igual a: 

```{r}
alpha = 0.05
z <- qnorm((1-alpha),mean = 0, sd = 1)  

VaRNormEst <- miu + sigma*z
VaRNormEst <- as.numeric(VaRNormEst)
```

An√°logamente al caso del VaR, se tiene que el **ES** por este m√©todo es igual a:

$ES_{(1-\alpha)}=\mu + \sigma\frac{\phi(\Phi^{-1}(1-\alpha))}{\alpha}$

Donde $(1-\alpha)$ es el nivel de confianza, $\Phi^{-1}$ es la inversa generalizada de la funci√≥n de distribuci√≥n normal est√°ndar o la funci√≥n cuantil, y $\phi$ es la funci√≥n de densidad normal est√°ndar.

Entonces, con nuestros datos el **ES est√°tico** ser√≠a igual a:

```{r}
df <- dnorm(z)

ESNormEst <-miu + sigma*(df/alpha)  
ESNormEst <- as.numeric(ESNormEst)
```

#### Condicional 

Es posible mejorar la estimaci√≥n del m√©todo bajo normalidad mediante c√°lculos m√°s refinados del pron√≥stico de la media y la varianza, generalmente a trav√©s de modelos **ARIMA** para la media y **EWMA** o **GARCH** para la varianza. 

De esta forma se obtiene un **pron√≥stico condicional** para el c√°lculo del **VaR** que est√° dado por:

$VaR_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}\Phi^{-1}(1-\alpha)$

Donde el sub√≠ndice de ${t+1}$ implica que el VaR es condicional a las estimaciones para el per√≠odo ${t+1}$.

As√≠, el **VaR condicional** por el m√©todo de normalidad ser√≠a igual a: 

```{r}
alpha = 0.05
z <- qnorm((1-alpha),mean = 0, sd = 1)  

VaRNormCon <- miut1 + sigmat1*z
```
La versi√≥n **condicional** del **ES** tambi√©n consiste en reemplazar la media y la varianza por los momentos condicionados a la informaci√≥n disponible hasta el per√≠odo $t$:

$ES_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}\frac{\phi(\Phi^{-1}(1-\alpha))}{\alpha}$

Entonces, con nuestros datos el **ES condicional** ser√≠a igual a:

```{r}
df <- dnorm(z)

ESNormCon <-miut1 + sigmat1*(df/alpha)  
```

### M√©todo de Simulaci√≥n Montecarlo

Este m√©todo hace uso de la **simulaci√≥n** de una distribuci√≥n te√≥rica **param√©trica** para realizar la estimaci√≥n del riesgo. El m√©todo consiste en la elecci√≥n de una funci√≥n de p√©rdidas te√≥rica y la calibraci√≥n de √©sta de acuerdo con los datos hist√≥ricos de los factores de riesgo.

#### Algoritmo de Estimaci√≥n: 

1. Elecci√≥n de la funci√≥n te√≥rica sobre la cual se realiza la simulaci√≥n.

2. Simulaci√≥n de n√∫meros aleatorios que se distribuyen seg√∫n la distribuci√≥n te√≥rica escogida (usualmente el n√∫mero de r√©plicas que se realiza es mayor que el n√∫mero de datos de cada factor de riesgo, para lograr una mayor precisi√≥n en la estimaci√≥n).

3. Construcci√≥n de una ‚Äúfunci√≥n de p√©rdidas‚Äù con los datos simulados, an√°loga al m√©todo hist√≥rico.

4. Estimaci√≥n del percentil asociado con el nivel de significancia ùú∂ escogido para el VaR, mediante el uso de la funci√≥n de p√©rdidas.

Cuando la funci√≥n te√≥rica escogida es la **normal**, el **VaR** bajo este m√©todo se resume como:

$VaR_{(1-\alpha)}=\mu + \sigma(q_{(1-\alpha)}(Z))$ donde $Z\sim N(0,1).$

```{r}
x1 <- rnorm(1000, mean = 0, sd = 1)
L1 <- x1*-1

VaRMontEst = miu + sigma*quantile(L1,(1-alpha))
```

La versi√≥n **din√°mica** consiste en reemplazar los pron√≥sticos est√°ticos de la media y la desviaci√≥n est√°ndar por sus versiones condicionales:

$VaR_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}(q_{(1-\alpha)}(Z))$ donde $Z\sim N(0,1).$

```{r}
VaRMontCon = miut1 + sigmat1*quantile(L1,(1-alpha))
```

El **ES** se calcula obteniendo el promedio de los retornos por encima del VaR (como en el caso de la simulaci√≥n hist√≥rica), pero tambi√©n podemos obtener una medida est√°tica o condicional. 

```{r}
umbral <- as.numeric(quantile(L1,(1-alpha)))
L1 <- as.data.frame(L1)

q <- filter(L1,L1>umbral)%>%
  summarise(mean(`L1`))

ESMontEst <- miu + sigma*q[1]
ESMontEst <- as.numeric(ESMontEst)

ESMontCon <- miut1 + sigmat1*q[1]
ESMontCon <- as.numeric(ESMontCon)

```
### M√©todo de Teor√≠a del Valor Extremo

La **Teor√≠a del Valor Extremo (EVT)** permite calcular un VaR m√°s acorde con algunos hechos estilizados de las series financieras, en particular con la forma leptoc√∫rtica de las colas de la funci√≥n de p√©rdidas. Siguiendo la metodolog√≠a de **POT (peaks over the threshold)**, los valores extremos que est√°n por encima de cierto umbral (los retornos que est√°n en las colas), se distribuyen de acuerdo a una distribuci√≥n **Generalizada de Pareto (GDP)**. 

Para este caso la ecuaci√≥n del **VaR** es igual a: 

$VaR_{(1-\alpha)}=q_{(1-\alpha)}(Z)=\upsilon+\frac{\beta}{\xi}\left[(\frac{\alpha}{T_{\upsilon}/T})^{-\xi}-1\right]$

Donde $(1-\alpha)$ es el nivel de confianza, $\upsilon$ es el umbral, $\beta$ es el par√°metro de escala, $\xi$ es el par√°metro de forma, $T$ es el n√∫mero total de observaciones, y $T_{\upsilon}$ es el n√∫mero de valores extremos. 

As√≠, el **VaR est√°tico y condicional** son iguales a: 

$VaR_{(1-\alpha)}=\mu + \sigma q_{(1-\alpha)}(Z)$ 

$VaR_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1} q_{(1-\alpha)}(Z)$

Para estimar el **VaR** primero debemos partir de datos i.id, es decir, debemos trabajar con los residuales del modelo ARIMA estandarizados por la volatilidad obtenida del modelo en varianza. 

```{r}
res.garch <- fitted.values(modelo.vol)
res.garch <- res.garch[2:m,1]
res <- res[2:m]

res.est <- res/res.garch

```
Ahora procedemos a modelar las colas de la distrubici√≥n de los residuales estandarizados:

```{r}
tailFraction <- 0.05 

TVE <- gpd.fit(res.est, threshold=quantile(res.est, (1-tailFraction)))

beta <- TVE[["mle"]][1] #Escala
eps  <- TVE[["mle"]][2] #Forma

v    <- quantile(res.est,(1-tailFraction))
N    <- length(res.est)  
Tv   <- round(N*tailFraction) 
```
As√≠, el *VaR* est√°tico y condiconal con la metodolog√≠a de *TVE* ser√≠a igual a:

```{r}
qTVE   <- v + (beta/eps)*((((1-tailFraction)/(Tv/N))^(-eps))-1)

VaRTVEEst <- miu + sigma*qTVE  

VaRTVECon <- miut1 + sigmat1*qTVE       
```
De acuerdo con las estimaciones realizadas de la distribuci√≥n en las colas, el **ES** es igual a: 

$ES_{(1-\alpha)}=\frac{q_{(1-\alpha)}}{1-\xi} + \frac{\beta-\xi\upsilon}{1-\xi}$

Donde $(1-\alpha)$ es el nivel de confianza, $\upsilon$ es el umbral, $\beta$ es el par√°metro de escala, $\xi$ es el par√°metro de forma. 

El caso **condicional** ser√≠a igual a:

$ES_{(1-\alpha),t+1}=\mu_{t+1} + \sigma_{t+1}ES_{(1-\alpha)}$

Con nuestros datos:

```{r}
ESTVE <- (qTVE/(1-eps))+((beta-eps*v)/(1-eps))

ESTVEEst <- miu + sigma*ESTVE
ESTVEEst <- as.numeric(ESTVEEst)

ESTVECon <- miut1 + sigmat1*ESTVE
```

#### Resumen

```{r}
ResumenVaR <- rbind(VaRHist*100,VaRNormEst*100,VaRNormCon*100,VaRMontEst*100,VaRMontCon*100,VaRTVEEst*100,VaRTVECon*100)

ResumenES <- rbind(ESHist*100,ESNormEst*100,ESNormCon*100,ESMontEst*100,ESMontCon*100,ESTVEEst*100,ESTVECon*100)

Resumen <- cbind(ResumenVaR,ResumenES) 

rownames(Resumen) <- c("Hist√≥rico","Normalidad Est√°tico","Normalidad Condicional","Montecarlo Est√°tico","Montecarlo Condicional","TVE Est√°tico","TVE Condicional")
colnames(Resumen) <- c("VaR","ES")

Resumen %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")
```

## VaR y Backtesting de un Portafolio

Aunque modelar el riesgo agregado de un portafolio directamente es √∫til para realizar una gesti√≥n del riesgo pasiva, **no es √∫til para una gesti√≥n activa**. Si queremos hacer **an√°lisis de sensibilidad** para evaluar los beneficios de la diversificaci√≥n, debemos modelar la dependencia entre los retornos individuales. En otras palabras,**¬°Necesitamos modelar la matriz de Varianza-Covarianza!**

Al igual que en el caso de la modelaci√≥n de la varianza, tenemos dos opciones para esta matriz: asumir que la **covarianza** entre los activos del portafolios es constante en el tiempo o que es **din√°mica**. 

En el siguiente ejemplo, vamos a calcular la matriz de varianza-covarianza haciendo uso de la metodlog√≠a propuesta por Engle en 2002 llamada *Dynamic Conditional Correlation (DCC)*. Bajo esta metodolog√≠a se modela la **correlaci√≥n** en vez de la varianza. Esto est√° motivado por la necesidad de eliminar la restricci√≥n de que varianzas y covarianzas deban tener los mismos par√°metros de persistencia. 

Emp√≠ricamente, se ha evidenciado que las correlaciones se incrementan en momentos de incertidumbre financiera haciendo que el riesgo se incremente a√∫n m√°s. 

As√≠, primero debemos estimar la volatilidad condicional de cada activo a trav√©s de un modelo GARCH o EWMA. Luego, estandarizamos cada retorno por su desviaci√≥n din√°mica. Posteriormente, calculamos la correlaci√≥n condicional entre cada par de activos, considerando una especificaci√≥n de tipo GARCH(1,1): 

$q_{ij,t+1}=p_{ij}+\alpha(z_{i,t}z_{j,t}-p_{ij})+\beta(q_{ij,t}-p_{ij})$

donde la din√°mica de las correlaciones est√° descrita por $q_{ij,t+1}$, y √©sta se actualiza con los retornos estandarizados $z_{i,t}$ y $z_{j,t}$.

### DCC-GARCH

Primero cargamos las librer√≠as que vamos a utilizar:

```{r, warning=FALSE, message=FALSE}
library(xts)
library(zoo)
library(quantmod) 
library(stochvol) 
library(robust)
library(psych)
library(ggplot2)
library(timeSeries)
library(tseries)
library(knitr)
library(kableExtra)
library(dplyr)
library(rmgarch)
library(FinTS)
library(rugarch)
library(e1071)
```

Luego definimos las fechas que vamos a tomar: 

```{r message=FALSE}
startt=c('2017/01/01')
endd=c('2021/12/01')
```
Ahora vamos a descargar el S&P 500 y las primeras 10 acciones del √≠ndice con mayor capitalizaci√≥n burs√°til a enero de 2022: 

```{r warning=FALSE, message=FALSE}
#S&P500
getSymbols("^GSPC",src='yahoo', from= startt , to = endd); sp1<-na.locf(GSPC$GSPC.Adjusted)  

#1.Apple
getSymbols("AAPL",src='yahoo', from= startt , to = endd);sp2<-na.locf(AAPL$AAPL.Adjusted) 

#2.Microsoft
getSymbols("MSFT",src='yahoo', from= startt , to = endd); sp3<-na.locf(MSFT$MSFT.Adjusted)  

#3.Amazon
getSymbols("AMZN",src='yahoo', from= startt , to = endd); sp4<-na.locf(AMZN$AMZN.Adjusted)  

#4.Alphabet Class A
getSymbols("GOOGL",src='yahoo', from= startt , to = endd); sp5<-na.locf(GOOGL$GOOGL.Adjusted)

#5.Tesla
getSymbols("TSLA",src='yahoo', from= startt , to = endd); sp6<-na.locf(TSLA$TSLA.Adjusted) 

#6.Alphabet Class C
getSymbols("GOOG",src='yahoo', from= startt , to = endd); sp7<-na.locf(GOOG$GOOG.Adjusted)

#7.Meta
getSymbols("FB",src='yahoo', from= startt , to = endd); sp8<-na.locf(FB$FB.Adjusted) 

#8.NVIDIA Corporation
getSymbols("NVDA",src='yahoo', from= startt , to = endd); sp9<-na.locf(NVDA$NVDA.Adjusted)

#9.Berkshire Hathaway Inc
getSymbols("BRK-B",src='yahoo', from= startt , to = endd); sp10<-na.locf(`BRK-B`$`BRK-B.Adjusted`) 

#UnitedHealth Group
getSymbols("UNH",src='yahoo', from= startt , to = endd); sp11<-na.locf(UNH$UNH.Adjusted)

#Tbill 13 weeks
getSymbols("^IRX",src='yahoo', from= startt , to = endd); sp12<-na.locf(IRX$IRX.Adjusted)     
```
En la siguiente parte unimos todas las series en una misma matriz y obtenemos los retornos:

```{r}
sp <- merge(sp1,sp2,fill=na.locf)
sp <- merge(sp,sp3,fill=na.locf)
sp <- merge(sp,sp4,fill=na.locf)
sp <- merge(sp,sp5,fill=na.locf)
sp <- merge(sp,sp6,fill=na.locf)
sp <- merge(sp,sp7,fill=na.locf)
sp <- merge(sp,sp8,fill=na.locf)
sp <- merge(sp,sp9,fill=na.locf)
sp <- merge(sp,sp10,fill=na.locf)
sp <- merge(sp,sp11,fill=na.locf)

sm<-sp

ret<- diff(log(sm))*100
ret<-na.locf(ret)
ret[is.na(ret)] <- 0
```
Vamos a conformar un portafolio con estas 10 acciones, asumiendo que cada una tiene el mismo peso. Nuestro objetivo es obtener la volatilidad del portafolio haciendo uso de la metodolog√≠a **DCC-GARCH**.

**Paso 1: Modelaci√≥n GARCH**

```{r}
uspec = ugarchspec(mean.model=list(armaOrder=c(0,0)),
                   variance.model=list(garchOrder=c(1,1),
                   model="sGARCH"),distribution.model="std")

```

**Paso 2: Espeficaci√≥n DCC**

```{r}
n = ncol(ret)-1

spec1= dccspec(uspec=multispec(replicate(n,uspec)),
               dccOrder=c(1,1),distribution="mvt")
```

**Paso 3: Estimaci√≥n**

```{r}
ret = ret[,2:11]

fit = dccfit(spec1,data=ret)
```

**Paso 4: Pron√≥stico**

```{r}
Pron = dccforecast(fit,n.ahead=1)

H = Pron@model[["H"]]
```

### Portafolio

Ahora calculamos la volatilidad del portafolio de 10 acciones,a partir de la matriz de varianza-covarianza obtenida por el modelo DCC-GARCH: 

```{r}
t=nrow(ret)

sigmaP = matrix(0,t,1)
w = matrix(0.1,1,n) #1x10
wT =matrix(0.1,n,1) #10x1

for (i in 1:t) {
  Hd=H[,,i]
  wH = w%*%Hd
  wHwT = wH%*%wT
  sigmaP[i,1]=sqrt(wHwT)
}
```
Los retornos de este portafolio con igual peso para todas las acciones ser√≠an igual a:

```{r}
RetP = matrix(,t,1)
w = as.vector(w)

for (i in 1:t) {
  retd = as.vector(ret[i,])
  RetP[i,1] = crossprod(retd,w)
}
```
Los retornos estandarizados por el modelo de volatilidad estimado ser√≠an igual a:

```{r}
Sigmas=fit@model[["sigma"]]

RetS = ret/Sigmas

RetPS = matrix(,t,1)
w = as.vector(w)

for (i in 1:t) {
  retsd = as.vector(RetS[i,])
  RetPS[i,1] = crossprod(retsd,w)
}
```
### Estimaci√≥n del VaR

Ahora calculamos el VaR del portafolio por el m√©todo de normalidad y el hist√≥rico a un d√≠a con un nivel de confianza del 99%.

#### VaR Normalidad

```{r}
conf = 0.99

q= qnorm(conf,mean=0,sd=1)

VaRN = matrix(,t,1)

for (i in 1:t) {
  VaRN[i,1]=sigmaP[i,1]*q
}
```
#### VaR Hist√≥rico

```{r}
m=t-249
VaRH = matrix(,m,1)

for (i in 1:m) {
  VaRH[i,1] = quantile(RetP[i:(249+i),1],probs=0.01)*-1
}
```

### Backtesting

El backtesting consiste en probar si efectivamente se cumple que los retornos (o residuales estandarizados) sobrepasan el pron√≥stico de la medida del riesgo el $\alpha*100$% del tiempo, como lo promete el $VaR_{(1-\alpha),t+1}$  (*unconditional coverage test*). 

Adem√°s, tambi√©n se debe probar si las observaciones que exceden al VaR no est√°n agrupadas (*independence test*). 

A continuaci√≥n, se presenta el Backtesting de los VaR calculados anteriormente. 

#### Backtesting VaR Normalidad 

```{r}
t2=t-1

VaRN2 = VaRN[1:t2,1]
RetPS2 = RetPS[2:t,1]
Backtest = matrix(,2,4)

Backtest[1,1]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$expected.exceed
Backtest[1,2]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$actual.exceed
Backtest[1,3]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$uc.Decision
Backtest[1,4]=VaRTest(alpha=0.01,actual=RetPS2,VaR=VaRN2*(-1))$cc.Decision

```

#### Backtesting VaR Hist√≥rico

```{r}
VaRH2 = VaRH[1:(m-1)]
RetP3 = RetP[251:t,1]

Backtest[2,1]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$expected.exceed
Backtest[2,2]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$actual.exceed
Backtest[2,3]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$uc.Decision
Backtest[2,4]=VaRTest(alpha=0.01,actual=RetP3,VaR=VaRH2*(-1))$cc.Decision

```

#### Resumen

```{r}
rownames(Backtest) <- c("DCC-GARCH Normal","Historical")
colnames(Backtest) <- c("Expected exceedances","Actual exceedances","uc.Decision","cc.Decision")

Backtest %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")
```

# Riesgo de Cr√©dito

El **riesgo de cr√©dito** se define como la p√©rdida potencial que se registra con motivo del incumplimiento de una contraparte en una transacci√≥n financiera. Tambi√©n se concibe como un **deterioro** en la calidad crediticia de la contraparte o en la garant√≠a o colateral pactada originalmente. Tradicionalmente, los bancos y empresas han elaborados procedimientos o normas de cr√©dito para seleccionar los clientes a quienes prestarles, montos y tasas. 

Los modelos m√°s utilizados en el mercado para medir riesgo de cr√©dito son: 

1. **Modelos econom√©tricos:** se utilizan modelos de regresi√≥n m√∫ltiple y modelos Logit y Probit para determinar las probabilidades de default. Las variables independientes son indicadores financieros o caracter√≠sticas del cliente, y variables externas que miden el comportamiento macroecon√≥mico.
 
2. **Modelo de Merton y Moody‚Äôs KMV:** se basa en la teor√≠a de opciones para calcular la probabilidad de default y el valor de la deuda. 

A continuaci√≥n veremos ejemplos de diferentes modelos econom√©tricos para calcular la probabilidad de default de un cliente de un banco. 

Cargamos las librer√≠as que vamos a utilizar:

```{r, warning=FALSE,message=FALSE}
library(caret)
library(dplyr)
library(e1071)
library(MASS)
library(MLmetrics)
library(readr)
library(rpart)
library(UBL)
library(kableExtra)
```

Importamos la base de datos crediticia y especificamos las variables categ√≥ricas como factores: 

```{r}
dd = read.csv("CleanCreditScoring.csv", header=TRUE, stringsAsFactors=FALSE)

dd$Status = as.factor(dd$Status)
dd$Home = as.factor(dd$Home)
dd$Marital = as.factor(dd$Marital)
dd$Records = as.factor(dd$Records)
dd$Job = as.factor(dd$Job)

summary(dd)
```

Ahora dividimos los datos en 2/3 para el aprendizaje del modelo y 1/3 para la validaci√≥n de √©ste.

```{r}
set.seed(100)
n <- nrow(dd)
learn <- sample(1:n, size=round(0.67 * n))
train <- dd[learn,]
test  <- dd[-learn,]
```

## Regresi√≥n Log√≠stica

```{r}
set.seed(10116070)

logmodel = glm(Status~., train, family = "binomial")
summary(logmodel)
prob_logpred = predict(logmodel,test,type = "response")
logpred = ifelse(prob_logpred > 0.5, "1", "0") 

confmatrix <- table(logpred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionLR <- (TP/(TP+FP))
RecallLR <- (TP/(TP+FN))
F1LR <- 2*(PrecisionLR*RecallLR/(PrecisionLR+RecallLR))
AccuracyLR <- (TP+TN)/(TN+TP+FN+TP)

SummaryLR <- rbind(PrecisionLR,RecallLR,F1LR,AccuracyLR)
```

## Support Vector Machine

```{r}
set.seed(10116070)
svm_model <- svm(Status ~ ., data=train)
svm_pred <- predict(svm_model,test)

confmatrix <- table(svm_pred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionSVM <- (TP/(TP+FP))
RecallSVM <- (TP/(TP+FN))
F1SVM <- 2*(PrecisionSVM*RecallSVM/(PrecisionSVM+RecallSVM))
AccuracySVM <- (TP+TN)/(TN+TP+FN+TP)

SummarySVM <- rbind(PrecisionSVM,RecallSVM,F1SVM,AccuracySVM)
```

## √Årbol de Decisi√≥n

```{r}
set.seed(10116070)
tree_model = rpart(Status ~ ., data = train, method = 'class')
tree_pred = as.data.frame(predict(tree_model,test))
tree_pred = tree_pred[,2]
tree_pred = ifelse(tree_pred > 0.5, "1", "0") 

confmatrix <- table(tree_pred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionDT <- (TP/(TP+FP))
RecallDT <- (TP/(TP+FN))
F1DT <- 2*(PrecisionDT*RecallDT/(PrecisionDT+RecallDT))
AccuracyDT <- (TP+TN)/(TN+TP+FN+TP)

SummaryDT <- rbind(PrecisionDT,RecallDT,F1DT,AccuracyDT)
```

## Nearest Neighbour

```{r}
set.seed(10116070)
trCtrl = trainControl(method = "repeatedcv", number = 10, repeats = 3 )
nnmodel = train(Status~.,data=train,method = 'knn',tuneLength = 20, trControl = trCtrl)
nnpred = predict(nnmodel,test)
table(nnpred,test$Status)

confmatrix <- table(nnpred,test$Status)
TN <- confmatrix[1,1]
FP <- confmatrix[1,2]
FN <- confmatrix[2,1]
TP <- confmatrix[2,2]

PrecisionNN <- (TP/(TP+FP))
RecallNN <- (TP/(TP+FN))
F1NN <- 2*(PrecisionNN*RecallNN/(PrecisionNN+RecallNN))
AccuracyNN <- (TP+TN)/(TN+TP+FN+TP)

SummaryNN <- rbind(PrecisionNN,RecallNN,F1NN,AccuracyNN)
```

#### Resumen

```{r}
Resumen <- cbind(SummaryLR,SummarySVM,SummaryDT,SummaryNN)

rownames(Resumen) <- c("Precision","Recall","F1","Accuracy")
colnames(Resumen) <- c("Logist Regression","Support Vector Machine",
                       "Decision Tree","Nearest Neighbour")

Resumen %>%
  kable(digits=2) %>%
  kable_styling("striped", full_width = F, position = "center")

```

# Riesgo Operacional

## ¬øQu√© es el riesgo operacional?

Es una categor√≠a de riesgo amplia que incluye desde la posibilidad de fallas en los procesos y fraudes hasta desastres naturales. En otras palabras, tener muchos d√≠as realmente malos. 

Para medir el riesgo operacional debemos calcular la `frecuencia` y `severidad`. Existe una analog√≠a al riesgo de cr√©dito, donde la frecuencia es es la probabilidad de default y la severidad es la proporci√≥n no recuperable de la deuda. En riesgo operacional, pensamos en la probabilidad de una p√©rdida como con qu√© frecuencia puede ocurrir esta p√©rdida. 

En cuanto a la severidad, pensamos en la p√©rdida monetaria. Estas p√©rdidas suelen estar por encima de un `threshold`, por lo que debemos enfocarnos en los valores extremos. 

En la mayor√≠a de las organizaciones no existe series de tiempo o de corte transversal con buena informaci√≥n sobre los eventos de riesgo operacional. Esto se debe principalmente a que muchos eventos de riesgo operacional nunca han sucedido antes. 

#### Un ejemplo

Suponga que la administraci√≥n de una empresa, despu√©s de mucha discusi√≥n con expertos, determina que: 

1. El promedio de las p√©rdidas debido a ataques cibern√©ticos es de $60 millones
2. Con una frecuencia promedio del 25% 
3. En un per√≠odo de 12 meses
4. La desviaci√≥n est√°ndar de las estimaciones es de $20 millones

### ¬øCu√°nto puedo perder?

Los managers pueden utilizar diferentes distribuciones de probabilidad para expresar sus preferencias sobre el riesgo de una p√©rdida. Una de estas es la distribuci√≥n `gamma` como funci√≥n de severidad. Esta distribuci√≥n permite colas pesadas y asimetr√≠a. 

La distribuci√≥n `gamma` est√° especificada s√≥lo con los dos primeros momentos: media y varianza. La probabilidad de que una p√©rdida sea igual a $x$ es:

$$p(x)= \frac{\beta^{\alpha}x^{\alpha-1}e^{-x\beta}}{\Gamma(\alpha)}$$
donde $\beta=\mu/\sigma^{2}$ y $\alpha=\mu\beta$.

Utilicemos entonces esta distribuci√≥n en nuestro ejemplo para simular la severidad de las p√©rdidas:

```{r warning=TRUE}
set.seed(1004)
n.sim <- 1000
mu <- 60     ## el promedio
sigma <- 20  ## qu√© tan inciertas piensa la administraci√≥n que son las estimaciones 
sigma.sq <- sigma^2
beta <- mu/sigma.sq
alpha <- beta * mu
severity <- rgamma(n.sim, alpha, beta)
summary(severity)
```


La distribuci√≥n se dispersa desde un m√≠nimo de 12 millones a un m√°ximo de m√°s de 150 millones. Asumiendo que la adminsitraci√≥n piensa que este m√≠nimo y m√°ximo son razonables, gr√°fiquemos la distribuci√≥n. 

Creamos un data frame en `gamma.sev` para crear los valores a utilizar en el `ggplot`:

```{r warning=TRUE}
library(ggplot2)
gamma.sev <- data.frame(Severity = severity, 
    Distribution = rep("Gamma", each = n.sim))
ggplot(gamma.sev, aes(x = Severity, fill = Distribution)) + 
    geom_density(alpha = 0.3)
```

Podemos agregar thresholds basados en el valor en riesgo (`VaR`) y el expected shortfall (`ES`). Calculamos las dos medidas de riesgo para un nivel de confianza de $1-\alpha$.

```{r}
alpha.tolerance <- 0.05

(VaR.sev <- quantile(severity, 1 - alpha.tolerance))

(ES.sev <- mean(gamma.sev$Severity[gamma.sev$Severity > VaR.sev]))
```

Grafiquemos el VaR y el ES:

```{r}
ggplot(gamma.sev, aes(x = Severity, fill = Distribution)) + 
    geom_density(alpha = 0.3) + geom_vline(xintercept = VaR.sev, 
    color = "red") + geom_vline(xintercept = ES.sev, 
    color = "blue")
```

### ¬øQu√© tan seguido ocurren mis p√©rdidas?

Usualmente se modela la frecuencia de las p√©rdidas con una distribuci√≥n `poisson`. Es una distribuci√≥n que est√° definida por el par√°metro $\lambda$, es decir, por el n√∫mero de veces que se espera que ocurra el evento durante un intervalo dado de tiempo.

La probabilidad de que $x$ eventos de riesgo ocurran a una tasa de llegada de $\lambda$ es igual a: 

$$p(x)= \frac{\lambda^{x}e^{-\lambda}}{x!}$$
La administraci√≥n asume que $\lambda=0.25$. As√≠, podemos decir que se cree que un evento de ataque cibern√©tico ocurrir√° una vez cada 4 meses en los pr√≥ximos 12 meses. 

Vamos a simular `n.sim` eventos con una distribuic√≥n `rpois()` condicional a $\lambda=0.25$:

```{r}
n.sim <- 1000
lambda <- 0.25
frequency <- rpois(n.sim, lambda)
summary(frequency)
```

Tambi√©n podemos graficar la distribuci√≥n:

```{r}
poisson.freq <- data.frame(Frequency = frequency, 
    Distribution = rep("Poisson", each = n.sim))

ggplot(poisson.freq, aes(x = frequency, 
    fill = Distribution)) + geom_density(alpha = 0.3)
```
La gr√°fica nos muestra una versi√≥n suavizada del recuento de eventos discretos: la cantidad de eventos m√°s probable es cero, luego un evento, luego dos, luego tres eventos.

### ¬øCu√°l es la p√©rdida potencial?

Para calcular la p√©rdida potencial debemos combinar frecuencia y severidad. Necesitamos "convolucionar" las distribuciones de frecuencia y severidad para formar una distribuci√≥n de p√©rdidas. ‚ÄúConvolucionar‚Äù significa que para cada escenario de p√©rdida de d√≥lares cibern√©ticos simulado, vemos si el escenario ocurre o no, usando los escenarios de frecuencia de Poisson.

```{r warning=TRUE}
loss <- rpois(n.sim, severity * frequency)
summary(loss)
```

Algunas anotaciones:

1. Este c√≥digo toma cada una de las severidades distribuidas `gamma` (`n.sim` de ellas) y se pregunta con qu√© frecuencia ocurrir√° cada una de ellas (`lambda`).

2. Luego simula la frecuencia de estos eventos de riesgo escalados por la severidad `gamma` del tama√±o de la p√©rdida.

3. El resultado es una convoluci√≥n de todos las `n.sim` de las severidades con todas las `n.sim` de las frecuencias. 

Calculemos ahora las medidas de riesgo para las p√©rdidas potenciales:

```{r warning=TRUE}
loss.rf <- data.frame(Loss = loss, Distribution = rep("Potential Loss", 
    each = n.sim))

(VaR.loss.rf <- quantile(loss.rf$Loss, 1 - alpha.tolerance))

(ES.loss.rf <- mean(loss.rf$Loss[loss.rf$Loss > VaR.loss.rf]))
```
Grafiquemos:

```{r}
ggplot(loss.rf, aes(x = Loss, fill = Distribution)) + 
    geom_density(alpha = 0.3) + geom_vline(xintercept = VaR.loss.rf, 
    color = "red") + geom_vline(xintercept = ES.loss.rf, 
    color = "blue")
```

